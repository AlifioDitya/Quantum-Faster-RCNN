{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T20:22:35.023271Z",
     "iopub.status.busy": "2024-11-29T20:22:35.022450Z",
     "iopub.status.idle": "2024-11-29T20:22:35.035283Z",
     "shell.execute_reply": "2024-11-29T20:22:35.034407Z",
     "shell.execute_reply.started": "2024-11-29T20:22:35.023221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset_params\": {\n",
    "        \"im_train_path\": \"VOC2007/JPEGImages\",\n",
    "        \"ann_train_path\": \"VOC2007/Annotations\",\n",
    "        \"im_test_path\": \"VOC2007-test/JPEGImages\",\n",
    "        \"ann_test_path\": \"VOC2007-test/Annotations\",\n",
    "        \"num_classes\": 21,\n",
    "        \"max_training_samples\": 5000,\n",
    "        \"max_testing_samples\": 250,\n",
    "    },\n",
    "    \"model_params\": {\n",
    "        \"im_channels\": 3,\n",
    "        \"aspect_ratios\": [0.5, 1, 2],\n",
    "        \"scales\": [128, 256, 512],\n",
    "        \"min_im_size\": 600,\n",
    "        \"max_im_size\": 1000,\n",
    "        \"backbone_out_channels\": 512,\n",
    "        \"fc_inner_dim\": 4096,\n",
    "        \"rpn_bg_threshold\": 0.3,\n",
    "        \"rpn_fg_threshold\": 0.7,\n",
    "        \"rpn_nms_threshold\": 0.7,\n",
    "        \"rpn_train_prenms_topk\": 12000,\n",
    "        \"rpn_test_prenms_topk\": 6000,\n",
    "        \"rpn_train_topk\": 2000,\n",
    "        \"rpn_test_topk\": 300,\n",
    "        \"rpn_batch_size\": 256,\n",
    "        \"rpn_pos_fraction\": 0.5,\n",
    "        \"roi_iou_threshold\": 0.5,\n",
    "        \"roi_low_bg_iou\": 0.0,  # Increase to 0.1 for hard negative\n",
    "        \"roi_pool_size\": 7,\n",
    "        \"roi_nms_threshold\": 0.3,\n",
    "        \"roi_topk_detections\": 100,\n",
    "        \"roi_score_threshold\": 0.05,\n",
    "        \"roi_batch_size\": 128,\n",
    "        \"roi_pos_fraction\": 0.25,\n",
    "        \"quantum_head\": True\n",
    "    },\n",
    "    \"train_params\": {\n",
    "        \"task_name\": \"voc\",\n",
    "        \"seed\": 1111,\n",
    "        \"acc_steps\": 1,  # Increase to mimic >1 batch size\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr_steps\": [12, 16],\n",
    "        \"lr\": 0.01,\n",
    "        \"ckpt_name\": \"variant_sel.pth\"\n",
    "    },\n",
    "    \"quantum_params\": {\n",
    "        \"n_qubits\" : 4,\n",
    "        \"q_depth\" : 3,\n",
    "        \"q_delta\" : 0.01  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T20:22:35.037810Z",
     "iopub.status.busy": "2024-11-29T20:22:35.037443Z",
     "iopub.status.idle": "2024-11-29T20:22:39.225781Z",
     "shell.execute_reply": "2024-11-29T20:22:39.225032Z",
     "shell.execute_reply.started": "2024-11-29T20:22:35.037772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch.nn as nn\n",
    "from torchvision.models import VGG16_Weights\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T20:22:51.359682Z",
     "iopub.status.busy": "2024-11-29T20:22:51.359387Z",
     "iopub.status.idle": "2024-11-29T20:22:53.657202Z",
     "shell.execute_reply": "2024-11-29T20:22:53.656482Z",
     "shell.execute_reply.started": "2024-11-29T20:22:51.359655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T20:22:53.658476Z",
     "iopub.status.busy": "2024-11-29T20:22:53.658042Z",
     "iopub.status.idle": "2024-11-29T20:22:53.662742Z",
     "shell.execute_reply": "2024-11-29T20:22:53.661817Z",
     "shell.execute_reply.started": "2024-11-29T20:22:53.658451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T20:22:53.664164Z",
     "iopub.status.busy": "2024-11-29T20:22:53.663895Z",
     "iopub.status.idle": "2024-11-29T20:22:53.681786Z",
     "shell.execute_reply": "2024-11-29T20:22:53.680884Z",
     "shell.execute_reply.started": "2024-11-29T20:22:53.664139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "n_qubits = config[\"quantum_params\"][\"n_qubits\"]\n",
    "q_depth = config[\"quantum_params\"][\"q_depth\"]\n",
    "q_delta = config[\"quantum_params\"][\"q_delta\"]\n",
    "\n",
    "# Define the quantum device\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "weight_shapes = {\"weights\": (q_depth, n_qubits)}\n",
    "\n",
    "def H_layer(n_qubits):\n",
    "    \"\"\"Layer of single-qubit Hadamard gates.\"\"\"\n",
    "    for idx in range(n_qubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "def RY_layer(weights):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y-axis.\"\"\"\n",
    "    for idx, weight in enumerate(weights):\n",
    "        qml.RY(weight, wires=idx)\n",
    "\n",
    "def entangling_layer(n_qubits):\n",
    "    \"\"\"Layer of entangling gates (CNOTs).\"\"\"\n",
    "    for i in range(0, n_qubits - 1, 2):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, n_qubits - 1, 2):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def sel_circuit(q_input_features, q_weights_flat):\n",
    "    \"\"\"\n",
    "    Dressed Quantum Circuit using PennyLane's built-in StronglyEntanglingLayers template.\n",
    "    \"\"\"\n",
    "    # Reshape weights to (layers, qubits, 3) â†’ 3 angles (RX, RY, RZ) per qubit per layer\n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits, 3)\n",
    "\n",
    "    # Encode classical features via angle encoding\n",
    "    RY_layer(q_input_features)\n",
    "\n",
    "    # Apply Strongly Entangling Layers\n",
    "    qml.templates.StronglyEntanglingLayers(weights=q_weights, wires=range(n_qubits))\n",
    "\n",
    "    # Measure each qubit in the Z basis\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# Custom layer to integrate quantum processing into a PyTorch model\n",
    "class DressedQuantumNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DressedQuantumNet, self).__init__()\n",
    "        self.pre_net = nn.Linear(input_dim, n_qubits)\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits * 3))\n",
    "        self.post_net = nn.Linear(n_qubits, output_dim)\n",
    "        \n",
    "    def forward(self, input_features):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "        \n",
    "        # obtain the input features for the quantum circuit\n",
    "        # by reducing the feature dimension from 512 to 4\n",
    "        pre_out = self.pre_net(input_features)\n",
    "        q_in = torch.tanh(pre_out) * np.pi / 2.0\n",
    "        \n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, n_qubits)\n",
    "        q_out = q_out.to(device)\n",
    "        for elem in q_in:\n",
    "            q_out_elem = torch.hstack(sel_circuit(elem, self.q_params)).float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "        \n",
    "        # return the two-dimensional prediction from the postprocessing layer\n",
    "        return self.post_net(q_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-29T20:22:53.683005Z",
     "iopub.status.busy": "2024-11-29T20:22:53.682722Z",
     "iopub.status.idle": "2024-11-29T20:22:53.698636Z",
     "shell.execute_reply": "2024-11-29T20:22:53.697930Z",
     "shell.execute_reply.started": "2024-11-29T20:22:53.682981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_images_and_anns(im_dir, ann_dir, label2idx):\n",
    "    r\"\"\"\n",
    "    Method to get the xml files and for each file\n",
    "    get all the objects and their ground truth detection\n",
    "    information for the dataset\n",
    "    :param im_dir: Path of the images\n",
    "    :param ann_dir: Path of annotation xmlfiles\n",
    "    :param label2idx: Class Name to index mapping for dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    im_infos = []\n",
    "    for ann_file in tqdm(glob.glob(os.path.join(ann_dir, '*.xml'))):\n",
    "        im_info = {}\n",
    "        im_info['img_id'] = os.path.basename(ann_file).split('.xml')[0]\n",
    "        im_info['filename'] = os.path.join(im_dir, '{}.jpg'.format(im_info['img_id']))\n",
    "        ann_info = ET.parse(ann_file)\n",
    "        root = ann_info.getroot()\n",
    "        size = root.find('size')\n",
    "        width = int(size.find('width').text)\n",
    "        height = int(size.find('height').text)\n",
    "        im_info['width'] = width\n",
    "        im_info['height'] = height\n",
    "        detections = []\n",
    "        \n",
    "        for obj in ann_info.findall('object'):\n",
    "            det = {}\n",
    "            label = label2idx[obj.find('name').text]\n",
    "            bbox_info = obj.find('bndbox')\n",
    "            bbox = [\n",
    "                int(float(bbox_info.find('xmin').text))-1,\n",
    "                int(float(bbox_info.find('ymin').text))-1,\n",
    "                int(float(bbox_info.find('xmax').text))-1,\n",
    "                int(float(bbox_info.find('ymax').text))-1\n",
    "            ]\n",
    "            det['label'] = label\n",
    "            det['bbox'] = bbox\n",
    "            detections.append(det)\n",
    "        im_info['detections'] = detections\n",
    "        im_infos.append(im_info)\n",
    "    print('Total {} images found'.format(len(im_infos)))\n",
    "    return im_infos\n",
    "\n",
    "\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, split, im_dir, ann_dir, max_images):\n",
    "        self.split = split\n",
    "        self.im_dir = im_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        classes = [\n",
    "            'person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "            'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "            'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor'\n",
    "        ]\n",
    "        classes = sorted(classes)\n",
    "        classes = ['background'] + classes\n",
    "        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
    "        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n",
    "        print(self.idx2label)\n",
    "        \n",
    "        self.images_info = load_images_and_anns(im_dir, ann_dir, self.label2idx)\n",
    "        \n",
    "        if max_images is not None:\n",
    "            self.images_info = self.images_info[:max_images]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_info)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        im_info = self.images_info[index]\n",
    "        im = Image.open(im_info['filename'])\n",
    "        to_flip = False\n",
    "        if self.split == 'train' and random.random() < 0.5:\n",
    "            to_flip = True\n",
    "            im = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
    "        targets = {}\n",
    "        targets['bboxes'] = torch.as_tensor([detection['bbox'] for detection in im_info['detections']])\n",
    "        targets['labels'] = torch.as_tensor([detection['label'] for detection in im_info['detections']])\n",
    "        if to_flip:\n",
    "            for idx, box in enumerate(targets['bboxes']):\n",
    "                x1, y1, x2, y2 = box\n",
    "                w = x2 - x1\n",
    "                im_w = im_tensor.shape[-1]\n",
    "                x1 = im_w - x1 - w\n",
    "                x2 = x1 + w\n",
    "                targets['bboxes'][idx] = torch.as_tensor([x1, y1, x2, y2])\n",
    "        return im_tensor, targets, im_info['filename']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T20:22:53.700161Z",
     "iopub.status.busy": "2024-11-29T20:22:53.699869Z",
     "iopub.status.idle": "2024-11-29T20:22:53.816593Z",
     "shell.execute_reply": "2024-11-29T20:22:53.815897Z",
     "shell.execute_reply.started": "2024-11-29T20:22:53.700131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_iou(boxes1, boxes2):\n",
    "    r\"\"\"\n",
    "    IOU between two sets of boxes\n",
    "    :param boxes1: (Tensor of shape N x 4)\n",
    "    :param boxes2: (Tensor of shape M x 4)\n",
    "    :return: IOU matrix of shape N x M\n",
    "    \"\"\"\n",
    "    # Area of boxes (x2-x1)*(y2-y1)\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # (N,)\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # (M,)\n",
    "    \n",
    "    # Get top left x1,y1 coordinate\n",
    "    x_left = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # (N, M)\n",
    "    y_top = torch.max(boxes1[:, None, 1], boxes2[:, 1])  # (N, M)\n",
    "    \n",
    "    # Get bottom right x2,y2 coordinate\n",
    "    x_right = torch.min(boxes1[:, None, 2], boxes2[:, 2])  # (N, M)\n",
    "    y_bottom = torch.min(boxes1[:, None, 3], boxes2[:, 3])  # (N, M)\n",
    "    \n",
    "    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)  # (N, M)\n",
    "    union = area1[:, None] + area2 - intersection_area  # (N, M)\n",
    "    iou = intersection_area / union  # (N, M)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def boxes_to_transformation_targets(ground_truth_boxes, anchors_or_proposals):\n",
    "    r\"\"\"\n",
    "    Given all anchor boxes or proposals in image and their respective\n",
    "    ground truth assignments, we use the x1,y1,x2,y2 coordinates of them\n",
    "    to get tx,ty,tw,th transformation targets for all anchor boxes or proposals\n",
    "    :param ground_truth_boxes: (anchors_or_proposals_in_image, 4)\n",
    "        Ground truth box assignments for the anchors/proposals\n",
    "    :param anchors_or_proposals: (anchors_or_proposals_in_image, 4) Anchors/Proposal boxes\n",
    "    :return: regression_targets: (anchors_or_proposals_in_image, 4) transformation targets tx,ty,tw,th\n",
    "        for all anchors/proposal boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for anchors\n",
    "    widths = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    heights = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * widths\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * heights\n",
    "    \n",
    "    # Get center_x,center_y,w,h from x1,y1,x2,y2 for gt boxes\n",
    "    gt_widths = ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n",
    "    gt_heights = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n",
    "    gt_center_x = ground_truth_boxes[:, 0] + 0.5 * gt_widths\n",
    "    gt_center_y = ground_truth_boxes[:, 1] + 0.5 * gt_heights\n",
    "    \n",
    "    targets_dx = (gt_center_x - center_x) / widths\n",
    "    targets_dy = (gt_center_y - center_y) / heights\n",
    "    targets_dw = torch.log(gt_widths / widths)\n",
    "    targets_dh = torch.log(gt_heights / heights)\n",
    "    regression_targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n",
    "    return regression_targets\n",
    "\n",
    "\n",
    "def apply_regression_pred_to_anchors_or_proposals(box_transform_pred, anchors_or_proposals):\n",
    "    r\"\"\"\n",
    "    Given the transformation parameter predictions for all\n",
    "    input anchors or proposals, transform them accordingly\n",
    "    to generate predicted proposals or predicted boxes\n",
    "    :param box_transform_pred: (num_anchors_or_proposals, num_classes, 4)\n",
    "    :param anchors_or_proposals: (num_anchors_or_proposals, 4)\n",
    "    :return pred_boxes: (num_anchors_or_proposals, num_classes, 4)\n",
    "    \"\"\"\n",
    "    box_transform_pred = box_transform_pred.reshape(\n",
    "        box_transform_pred.size(0), -1, 4)\n",
    "    \n",
    "    # Get cx, cy, w, h from x1,y1,x2,y2\n",
    "    w = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    h = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * w\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * h\n",
    "    \n",
    "    dx = box_transform_pred[..., 0]\n",
    "    dy = box_transform_pred[..., 1]\n",
    "    dw = box_transform_pred[..., 2]\n",
    "    dh = box_transform_pred[..., 3]\n",
    "    # dh -> (num_anchors_or_proposals, num_classes)\n",
    "    \n",
    "    # Prevent sending too large values into torch.exp()\n",
    "    dw = torch.clamp(dw, max=math.log(1000.0 / 16))\n",
    "    dh = torch.clamp(dh, max=math.log(1000.0 / 16))\n",
    "    \n",
    "    pred_center_x = dx * w[:, None] + center_x[:, None]\n",
    "    pred_center_y = dy * h[:, None] + center_y[:, None]\n",
    "    pred_w = torch.exp(dw) * w[:, None]\n",
    "    pred_h = torch.exp(dh) * h[:, None]\n",
    "    # pred_center_x -> (num_anchors_or_proposals, num_classes)\n",
    "    \n",
    "    pred_box_x1 = pred_center_x - 0.5 * pred_w\n",
    "    pred_box_y1 = pred_center_y - 0.5 * pred_h\n",
    "    pred_box_x2 = pred_center_x + 0.5 * pred_w\n",
    "    pred_box_y2 = pred_center_y + 0.5 * pred_h\n",
    "    \n",
    "    pred_boxes = torch.stack((\n",
    "        pred_box_x1,\n",
    "        pred_box_y1,\n",
    "        pred_box_x2,\n",
    "        pred_box_y2),\n",
    "        dim=2)\n",
    "    # pred_boxes -> (num_anchors_or_proposals, num_classes, 4)\n",
    "    return pred_boxes\n",
    "\n",
    "\n",
    "def sample_positive_negative(labels, positive_count, total_count):\n",
    "    # Sample positive and negative proposals\n",
    "    positive = torch.where(labels >= 1)[0]\n",
    "    negative = torch.where(labels == 0)[0]\n",
    "    num_pos = positive_count\n",
    "    num_pos = min(positive.numel(), num_pos)\n",
    "    num_neg = total_count - num_pos\n",
    "    num_neg = min(negative.numel(), num_neg)\n",
    "    perm_positive_idxs = torch.randperm(positive.numel(),\n",
    "                                        device=positive.device)[:num_pos]\n",
    "    perm_negative_idxs = torch.randperm(negative.numel(),\n",
    "                                        device=negative.device)[:num_neg]\n",
    "    pos_idxs = positive[perm_positive_idxs]\n",
    "    neg_idxs = negative[perm_negative_idxs]\n",
    "    sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_pos_idx_mask[pos_idxs] = True\n",
    "    sampled_neg_idx_mask[neg_idxs] = True\n",
    "    return sampled_neg_idx_mask, sampled_pos_idx_mask\n",
    "\n",
    "\n",
    "def clamp_boxes_to_image_boundary(boxes, image_shape):\n",
    "    boxes_x1 = boxes[..., 0]\n",
    "    boxes_y1 = boxes[..., 1]\n",
    "    boxes_x2 = boxes[..., 2]\n",
    "    boxes_y2 = boxes[..., 3]\n",
    "    height, width = image_shape[-2:]\n",
    "    boxes_x1 = boxes_x1.clamp(min=0, max=width)\n",
    "    boxes_x2 = boxes_x2.clamp(min=0, max=width)\n",
    "    boxes_y1 = boxes_y1.clamp(min=0, max=height)\n",
    "    boxes_y2 = boxes_y2.clamp(min=0, max=height)\n",
    "    boxes = torch.cat((\n",
    "        boxes_x1[..., None],\n",
    "        boxes_y1[..., None],\n",
    "        boxes_x2[..., None],\n",
    "        boxes_y2[..., None]),\n",
    "        dim=-1)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def transform_boxes_to_original_size(boxes, new_size, original_size):\n",
    "    r\"\"\"\n",
    "    Boxes are for resized image (min_size=600, max_size=1000).\n",
    "    This method converts the boxes to whatever dimensions\n",
    "    the image was before resizing\n",
    "    :param boxes:\n",
    "    :param new_size:\n",
    "    :param original_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ratios = [\n",
    "        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
    "        / torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
    "        for s, s_orig in zip(new_size, original_size)\n",
    "    ]\n",
    "    ratio_height, ratio_width = ratios\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    xmin = xmin * ratio_width\n",
    "    xmax = xmax * ratio_width\n",
    "    ymin = ymin * ratio_height\n",
    "    ymax = ymax * ratio_height\n",
    "    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "\n",
    "\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    r\"\"\"\n",
    "    RPN with following layers on the feature map\n",
    "        1. 3x3 conv layer followed by Relu\n",
    "        2. 1x1 classification conv with num_anchors(num_scales x num_aspect_ratios) output channels\n",
    "        3. 1x1 classification conv with 4 x num_anchors output channels\n",
    "\n",
    "    Classification is done via one value indicating probability of foreground\n",
    "    with sigmoid applied during inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, scales, aspect_ratios, model_config):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.scales = scales\n",
    "        self.low_iou_threshold = model_config['rpn_bg_threshold']\n",
    "        self.high_iou_threshold = model_config['rpn_fg_threshold']\n",
    "        self.rpn_nms_threshold = model_config['rpn_nms_threshold']\n",
    "        self.rpn_batch_size = model_config['rpn_batch_size']\n",
    "        self.rpn_pos_count = int(model_config['rpn_pos_fraction'] * self.rpn_batch_size)\n",
    "        self.rpn_topk = model_config['rpn_train_topk'] if self.training else model_config['rpn_test_topk']\n",
    "        self.rpn_prenms_topk = model_config['rpn_train_prenms_topk'] if self.training \\\n",
    "            else model_config['rpn_test_prenms_topk']\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
    "        \n",
    "        # 3x3 conv layer\n",
    "        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 1x1 classification conv layer\n",
    "        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n",
    "        \n",
    "        # 1x1 regression\n",
    "        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
    "        \n",
    "        for layer in [self.rpn_conv, self.cls_layer, self.bbox_reg_layer]:\n",
    "            torch.nn.init.normal_(layer.weight, std=0.01)\n",
    "            torch.nn.init.constant_(layer.bias, 0)\n",
    "    \n",
    "    def generate_anchors(self, image, feat):\n",
    "        r\"\"\"\n",
    "        Method to generate anchors. First we generate one set of zero-centred anchors\n",
    "        using the scales and aspect ratios provided.\n",
    "        We then generate shift values in x,y axis for all featuremap locations.\n",
    "        The single zero centred anchors generated are replicated and shifted accordingly\n",
    "        to generate anchors for all feature map locations.\n",
    "        Note that these anchors are generated such that their centre is top left corner of the\n",
    "        feature map cell rather than the centre of the feature map cell.\n",
    "        :param image: (N, C, H, W) tensor\n",
    "        :param feat: (N, C_feat, H_feat, W_feat) tensor\n",
    "        :return: anchor boxes of shape (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        \"\"\"\n",
    "        grid_h, grid_w = feat.shape[-2:]\n",
    "        image_h, image_w = image.shape[-2:]\n",
    "        \n",
    "        # For the vgg16 case stride would be 16 for both h and w\n",
    "        stride_h = torch.tensor(image_h // grid_h, dtype=torch.int64, device=feat.device)\n",
    "        stride_w = torch.tensor(image_w // grid_w, dtype=torch.int64, device=feat.device)\n",
    "        \n",
    "        scales = torch.as_tensor(self.scales, dtype=feat.dtype, device=feat.device)\n",
    "        aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n",
    "        \n",
    "        # Assuming anchors of scale 128 sq pixels\n",
    "        # For 1:1 it would be (128, 128) -> area=16384\n",
    "        # For 2:1 it would be (181.02, 90.51) -> area=16384\n",
    "        # For 1:2 it would be (90.51, 181.02) -> area=16384\n",
    "        \n",
    "        # The below code ensures h/w = aspect_ratios and h*w=1\n",
    "        h_ratios = torch.sqrt(aspect_ratios)\n",
    "        w_ratios = 1 / h_ratios\n",
    "        \n",
    "        # Now we will just multiply h and w with scale(example 128)\n",
    "        # to make h*w = 128 sq pixels and h/w = aspect_ratios\n",
    "        # This gives us the widths and heights of all anchors\n",
    "        # which we need to replicate at all locations\n",
    "        ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        \n",
    "        # Now we make all anchors zero centred\n",
    "        # So x1, y1, x2, y2 = -w/2, -h/2, w/2, h/2\n",
    "        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "        base_anchors = base_anchors.round()\n",
    "        \n",
    "        # Get the shifts in x axis (0, 1,..., W_feat-1) * stride_w\n",
    "        shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n",
    "\n",
    "        # Get the shifts in x axis (0, 1,..., H_feat-1) * stride_h\n",
    "        shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n",
    "        \n",
    "        # Create a grid using these shifts\n",
    "        shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n",
    "        # shifts_x -> (H_feat, W_feat)\n",
    "        # shifts_y -> (H_feat, W_feat)\n",
    "        \n",
    "        shifts_x = shifts_x.reshape(-1)\n",
    "        shifts_y = shifts_y.reshape(-1)\n",
    "        # Setting shifts for x1 and x2(same as shifts_x) and y1 and y2(same as shifts_y)\n",
    "        shifts = torch.stack((shifts_x, shifts_y, shifts_x, shifts_y), dim=1)\n",
    "        # shifts -> (H_feat * W_feat, 4)\n",
    "        \n",
    "        # base_anchors -> (num_anchors_per_location, 4)\n",
    "        # shifts -> (H_feat * W_feat, 4)\n",
    "        # Add these shifts to each of the base anchors\n",
    "        anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4))\n",
    "        # anchors -> (H_feat * W_feat, num_anchors_per_location, 4)\n",
    "        anchors = anchors.reshape(-1, 4)\n",
    "        # anchors -> (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        return anchors\n",
    "    \n",
    "    def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
    "        r\"\"\"\n",
    "        For each anchor assign a ground truth box based on the IOU.\n",
    "        Also creates classification labels to be used for training\n",
    "        label=1 for anchors where maximum IOU with a gtbox > high_iou_threshold\n",
    "        label=0 for anchors where maximum IOU with a gtbox < low_iou_threshold\n",
    "        label=-1 for anchors where maximum IOU with a gtbox between (low_iou_threshold, high_iou_threshold)\n",
    "        :param anchors: (num_anchors_in_image, 4) all anchor boxes\n",
    "        :param gt_boxes: (num_gt_boxes_in_image, 4) all ground truth boxes\n",
    "        :return:\n",
    "            label: (num_anchors_in_image) {-1/0/1}\n",
    "            matched_gt_boxes: (num_anchors_in_image, 4) coordinates of assigned gt_box to each anchor\n",
    "                Even background/to_be_ignored anchors will be assigned some ground truth box.\n",
    "                It's fine, we will use label to differentiate those instances later\n",
    "        \"\"\"\n",
    "        # Get (gt_boxes, num_anchors_in_image) IOU matrix\n",
    "        iou_matrix = get_iou(gt_boxes, anchors)\n",
    "        \n",
    "        # For each anchor get the gt box index with maximum overlap\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        # best_match_gt_idx -> (num_anchors_in_image)\n",
    "        \n",
    "        # This copy of best_match_gt_idx will be needed later to\n",
    "        # add low quality matches\n",
    "        best_match_gt_idx_pre_thresholding = best_match_gt_idx.clone()\n",
    "        \n",
    "        # Based on threshold, update the values of best_match_gt_idx\n",
    "        # For anchors with highest IOU < low_threshold update to be -1\n",
    "        # For anchors with highest IOU between low_threshold & high threshold update to be -2\n",
    "        below_low_threshold = best_match_iou < self.low_iou_threshold\n",
    "        between_thresholds = (best_match_iou >= self.low_iou_threshold) & (best_match_iou < self.high_iou_threshold)\n",
    "        best_match_gt_idx[below_low_threshold] = -1\n",
    "        best_match_gt_idx[between_thresholds] = -2\n",
    "        \n",
    "        # Add low quality anchor boxes, if for a given ground truth box, these are the ones\n",
    "        # that have highest IOU with that gt box\n",
    "        \n",
    "        # For each gt box, get the maximum IOU value amongst all anchors\n",
    "        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
    "        # best_anchor_iou_for_gt -> (num_gt_boxes_in_image)\n",
    "        \n",
    "        # For each gt box get those anchors\n",
    "        # which have this same IOU as present in best_anchor_iou_for_gt\n",
    "        # This is to ensure if 10 anchors all have the same IOU value,\n",
    "        # which is equal to the highest IOU that this gt box has with any anchor\n",
    "        # then we get all these 10 anchors\n",
    "        gt_pred_pair_with_highest_iou = torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
    "        # gt_pred_pair_with_highest_iou -> [0, 0, 0, 1, 1, 1], [8896,  8905,  8914, 10472, 10805, 11138]\n",
    "        # This means that anchors at the first 3 indexes have an IOU with gt box at index 0\n",
    "        # which is equal to the highest IOU that this gt box has with ANY anchor\n",
    "        # Similarly anchor at last three indexes(10472, 10805, 11138) have an IOU with gt box at index 1\n",
    "        # which is equal to the highest IOU that this gt box has with ANY anchor\n",
    "        # These 6 anchor indexes will also be added as positive anchors\n",
    "        \n",
    "        # Get all the anchors indexes to update\n",
    "        pred_inds_to_update = gt_pred_pair_with_highest_iou[1]\n",
    "        \n",
    "        # Update the matched gt index for all these anchors with whatever was the best gt box\n",
    "        # prior to thresholding\n",
    "        best_match_gt_idx[pred_inds_to_update] = best_match_gt_idx_pre_thresholding[pred_inds_to_update]\n",
    "        \n",
    "        # best_match_gt_idx is either a valid index for all anchors or -1(background) or -2(to be ignored)\n",
    "        # Clamp this so that the best_match_gt_idx is a valid non-negative index\n",
    "        # At this moment the -1 and -2 labelled anchors will be mapped to the 0th gt box\n",
    "        matched_gt_boxes = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "        \n",
    "        # Set all foreground anchor labels as 1\n",
    "        labels = best_match_gt_idx >= 0\n",
    "        labels = labels.to(dtype=torch.float32)\n",
    "        \n",
    "        # Set all background anchor labels as 0\n",
    "        background_anchors = best_match_gt_idx == -1\n",
    "        labels[background_anchors] = 0.0\n",
    "        \n",
    "        # Set all to be ignored anchor labels as -1\n",
    "        ignored_anchors = best_match_gt_idx == -2\n",
    "        labels[ignored_anchors] = -1.0\n",
    "        # Later for classification we will only pick labels which have > 0 label\n",
    "        \n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def filter_proposals(self, proposals, cls_scores, image_shape):\n",
    "        r\"\"\"\n",
    "        This method does three kinds of filtering/modifications\n",
    "        1. Pre NMS topK filtering\n",
    "        2. Make proposals valid by clamping coordinates(0, width/height)\n",
    "        2. Small Boxes filtering based on width and height\n",
    "        3. NMS\n",
    "        4. Post NMS topK filtering\n",
    "        :param proposals: (num_anchors_in_image, 4)\n",
    "        :param cls_scores: (num_anchors_in_image, 4) these are cls logits\n",
    "        :param image_shape: resized image shape needed to clip proposals to image boundary\n",
    "        :return: proposals and cls_scores: (num_filtered_proposals, 4) and (num_filtered_proposals)\n",
    "        \"\"\"\n",
    "        # Pre NMS Filtering\n",
    "        cls_scores = cls_scores.reshape(-1)\n",
    "        cls_scores = torch.sigmoid(cls_scores)\n",
    "        _, top_n_idx = cls_scores.topk(min(self.rpn_prenms_topk, len(cls_scores)))\n",
    "        \n",
    "        cls_scores = cls_scores[top_n_idx]\n",
    "        proposals = proposals[top_n_idx]\n",
    "        ##################\n",
    "        \n",
    "        # Clamp boxes to image boundary\n",
    "        proposals = clamp_boxes_to_image_boundary(proposals, image_shape)\n",
    "        ####################\n",
    "        \n",
    "        # Small boxes based on width and height filtering\n",
    "        min_size = 16\n",
    "        ws, hs = proposals[:, 2] - proposals[:, 0], proposals[:, 3] - proposals[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        proposals = proposals[keep]\n",
    "        cls_scores = cls_scores[keep]\n",
    "        ####################\n",
    "        \n",
    "        # NMS based on objectness scores\n",
    "        keep_mask = torch.zeros_like(cls_scores, dtype=torch.bool)\n",
    "        keep_indices = torch.ops.torchvision.nms(proposals, cls_scores, self.rpn_nms_threshold)\n",
    "        keep_mask[keep_indices] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        # Sort by objectness\n",
    "        post_nms_keep_indices = keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n",
    "        \n",
    "        # Post NMS topk filtering\n",
    "        proposals, cls_scores = (proposals[post_nms_keep_indices[:self.rpn_topk]],\n",
    "                                 cls_scores[post_nms_keep_indices[:self.rpn_topk]])\n",
    "        \n",
    "        return proposals, cls_scores\n",
    "    \n",
    "    def forward(self, image, feat, target=None):\n",
    "        r\"\"\"\n",
    "        Main method for RPN does the following:\n",
    "        1. Call RPN specific conv layers to generate classification and\n",
    "            bbox transformation predictions for anchors\n",
    "        2. Generate anchors for entire image\n",
    "        3. Transform generated anchors based on predicted bbox transformation to generate proposals\n",
    "        4. Filter proposals\n",
    "        5. For training additionally we do the following:\n",
    "            a. Assign target ground truth labels and boxes to each anchors\n",
    "            b. Sample positive and negative anchors\n",
    "            c. Compute classification loss using sampled pos/neg anchors\n",
    "            d. Compute Localization loss using sampled pos anchors\n",
    "        :param image:\n",
    "        :param feat:\n",
    "        :param target:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Call RPN layers\n",
    "        rpn_feat = nn.ReLU()(self.rpn_conv(feat))\n",
    "        cls_scores = self.cls_layer(rpn_feat)\n",
    "        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n",
    "\n",
    "        # Generate anchors\n",
    "        anchors = self.generate_anchors(image, feat)\n",
    "        \n",
    "        # Reshape classification scores to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 1)\n",
    "        # cls_score -> (Batch_Size, Number of Anchors per location, H_feat, W_feat)\n",
    "        number_of_anchors_per_location = cls_scores.size(1)\n",
    "        cls_scores = cls_scores.permute(0, 2, 3, 1)\n",
    "        cls_scores = cls_scores.reshape(-1, 1)\n",
    "        # cls_score -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 1)\n",
    "        \n",
    "        # Reshape bbox predictions to be (Batch Size * H_feat * W_feat * Number of Anchors Per Location, 4)\n",
    "        # box_transform_pred -> (Batch_Size, Number of Anchors per location*4, H_feat, W_feat)\n",
    "        box_transform_pred = box_transform_pred.view(\n",
    "            box_transform_pred.size(0),\n",
    "            number_of_anchors_per_location,\n",
    "            4,\n",
    "            rpn_feat.shape[-2],\n",
    "            rpn_feat.shape[-1])\n",
    "        box_transform_pred = box_transform_pred.permute(0, 3, 4, 1, 2)\n",
    "        box_transform_pred = box_transform_pred.reshape(-1, 4)\n",
    "        # box_transform_pred -> (Batch_Size*H_feat*W_feat*Number of Anchors per location, 4)\n",
    "        \n",
    "        # Transform generated anchors according to box transformation prediction\n",
    "        proposals = apply_regression_pred_to_anchors_or_proposals(\n",
    "            box_transform_pred.detach().reshape(-1, 1, 4),\n",
    "            anchors)\n",
    "        proposals = proposals.reshape(proposals.size(0), 4)\n",
    "        ######################\n",
    "        \n",
    "        proposals, scores = self.filter_proposals(proposals, cls_scores.detach(), image.shape)\n",
    "        rpn_output = {\n",
    "            'proposals': proposals,\n",
    "            'scores': scores\n",
    "        }\n",
    "        if not self.training or target is None:\n",
    "            # If we are not training no need to do anything\n",
    "            return rpn_output\n",
    "        else:\n",
    "            # Assign gt box and label for each anchor\n",
    "            labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_targets_to_anchors(\n",
    "                anchors,\n",
    "                target['bboxes'][0])\n",
    "            \n",
    "            # Based on gt assignment above, get regression target for the anchors\n",
    "            # matched_gt_boxes_for_anchors -> (Number of anchors in image, 4)\n",
    "            # anchors -> (Number of anchors in image, 4)\n",
    "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_anchors, anchors)\n",
    "            \n",
    "            ####### Sampling positive and negative anchors ####\n",
    "            # Our labels were {fg:1, bg:0, to_be_ignored:-1}\n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n",
    "                labels_for_anchors,\n",
    "                positive_count=self.rpn_pos_count,\n",
    "                total_count=self.rpn_batch_size)\n",
    "            \n",
    "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "            \n",
    "            localization_loss = (\n",
    "                    torch.nn.functional.smooth_l1_loss(\n",
    "                        box_transform_pred[sampled_pos_idx_mask],\n",
    "                        regression_targets[sampled_pos_idx_mask],\n",
    "                        beta=1 / 9,\n",
    "                        reduction=\"sum\",\n",
    "                    )\n",
    "                    / (sampled_idxs.numel())\n",
    "            ) \n",
    "\n",
    "            cls_loss = torch.nn.functional.binary_cross_entropy_with_logits(cls_scores[sampled_idxs].flatten(),\n",
    "                                                                            labels_for_anchors[sampled_idxs].flatten())\n",
    "\n",
    "            rpn_output['rpn_classification_loss'] = cls_loss\n",
    "            rpn_output['rpn_localization_loss'] = localization_loss\n",
    "            return rpn_output\n",
    "\n",
    "\n",
    "class ROIHead(nn.Module):\n",
    "    r\"\"\"\n",
    "    ROI head on top of ROI pooling layer for generating\n",
    "    classification and box transformation predictions\n",
    "    We have two fc layers followed by a classification fc layer\n",
    "    and a bbox regression fc layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_config, num_classes, in_channels):\n",
    "        super(ROIHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.roi_batch_size = model_config['roi_batch_size']\n",
    "        self.roi_pos_count = int(model_config['roi_pos_fraction'] * self.roi_batch_size)\n",
    "        self.iou_threshold = model_config['roi_iou_threshold']\n",
    "        self.low_bg_iou = model_config['roi_low_bg_iou']\n",
    "        self.nms_threshold = model_config['roi_nms_threshold']\n",
    "        self.topK_detections = model_config['roi_topk_detections']\n",
    "        self.low_score_threshold = model_config['roi_score_threshold']\n",
    "        self.pool_size = model_config['roi_pool_size']\n",
    "        self.fc_inner_dim = model_config['fc_inner_dim']\n",
    "        \n",
    "        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n",
    "        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
    "\n",
    "        # Use quantum layers for classification and bounding box regression\n",
    "        if (config['model_params']['quantum_head']):\n",
    "            self.cls_layer = DressedQuantumNet(input_dim=self.fc_inner_dim, output_dim=num_classes)\n",
    "            self.cls_layer = self.cls_layer.to(device)\n",
    "            self.bbox_reg_layer = DressedQuantumNet(input_dim=self.fc_inner_dim, output_dim=num_classes * 4)\n",
    "            self.bbox_reg_layer = self.bbox_reg_layer.to(device)\n",
    "        else:\n",
    "            self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n",
    "            self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n",
    "            \n",
    "            torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
    "            torch.nn.init.constant_(self.cls_layer.bias, 0)\n",
    "\n",
    "            torch.nn.init.normal_(self.bbox_reg_layer.weight, std=0.001)\n",
    "            torch.nn.init.constant_(self.bbox_reg_layer.bias, 0)\n",
    "    \n",
    "    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
    "        r\"\"\"\n",
    "        Given a set of proposals and ground truth boxes and their respective labels.\n",
    "        Use IOU to assign these proposals to some gt box or background\n",
    "        :param proposals: (number_of_proposals, 4)\n",
    "        :param gt_boxes: (number_of_gt_boxes, 4)\n",
    "        :param gt_labels: (number_of_gt_boxes)\n",
    "        :return:\n",
    "            labels: (number_of_proposals)\n",
    "            matched_gt_boxes: (number_of_proposals, 4)\n",
    "        \"\"\"\n",
    "        # Get IOU Matrix between gt boxes and proposals\n",
    "        iou_matrix = get_iou(gt_boxes, proposals)\n",
    "        # For each gt box proposal find best matching gt box\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        background_proposals = (best_match_iou < self.iou_threshold) & (best_match_iou >= self.low_bg_iou)\n",
    "        ignored_proposals = best_match_iou < self.low_bg_iou\n",
    "        \n",
    "        # Update best match of low IOU proposals to -1\n",
    "        best_match_gt_idx[background_proposals] = -1\n",
    "        best_match_gt_idx[ignored_proposals] = -2\n",
    "        \n",
    "        # Get best marching gt boxes for ALL proposals\n",
    "        # Even background proposals would have a gt box assigned to it\n",
    "        # Label will be used to ignore them later\n",
    "        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "        \n",
    "        # Get class label for all proposals according to matching gt boxes\n",
    "        labels = gt_labels[best_match_gt_idx.clamp(min=0)]\n",
    "        labels = labels.to(dtype=torch.int64)\n",
    "        \n",
    "        # Update background proposals to be of label 0(background)\n",
    "        labels[background_proposals] = 0\n",
    "        \n",
    "        # Set all to be ignored anchor labels as -1(will be ignored)\n",
    "        labels[ignored_proposals] = -1\n",
    "        \n",
    "        return labels, matched_gt_boxes_for_proposals\n",
    "    \n",
    "    def forward(self, feat, proposals, image_shape, target):\n",
    "        r\"\"\"\n",
    "        Main method for ROI head that does the following:\n",
    "        1. If training assign target boxes and labels to all proposals\n",
    "        2. If training sample positive and negative proposals\n",
    "        3. If training get bbox transformation targets for all proposals based on assignments\n",
    "        4. Get ROI Pooled features for all proposals\n",
    "        5. Call fc6, fc7 and classification and bbox transformation fc layers\n",
    "        6. Compute classification and localization loss\n",
    "\n",
    "        :param feat:\n",
    "        :param proposals:\n",
    "        :param image_shape:\n",
    "        :param target:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.training and target is not None:\n",
    "            # Add ground truth to proposals\n",
    "            proposals = torch.cat([proposals, target['bboxes'][0]], dim=0)\n",
    "            \n",
    "            gt_boxes = target['bboxes'][0]\n",
    "            gt_labels = target['labels'][0]\n",
    "            \n",
    "            labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(proposals, gt_boxes, gt_labels)\n",
    "            \n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(labels,\n",
    "                                                                                  positive_count=self.roi_pos_count,\n",
    "                                                                                  total_count=self.roi_batch_size)\n",
    "            \n",
    "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "            \n",
    "            # Keep only sampled proposals\n",
    "            proposals = proposals[sampled_idxs]\n",
    "            labels = labels[sampled_idxs]\n",
    "            matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n",
    "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_proposals, proposals)\n",
    "            # regression_targets -> (sampled_training_proposals, 4)\n",
    "            # matched_gt_boxes_for_proposals -> (sampled_training_proposals, 4)\n",
    "        \n",
    "        # Get desired scale to pass to roi pooling function\n",
    "        # For vgg16 case this would be 1/16 (0.0625)\n",
    "        size = feat.shape[-2:]\n",
    "        possible_scales = []\n",
    "        for s1, s2 in zip(size, image_shape):\n",
    "            approx_scale = float(s1) / float(s2)\n",
    "            scale = 2 ** float(torch.tensor(approx_scale).log2().round())\n",
    "            possible_scales.append(scale)\n",
    "        assert possible_scales[0] == possible_scales[1]\n",
    "        \n",
    "        # ROI pooling and call all layers for prediction\n",
    "        proposal_roi_pool_feats = torchvision.ops.roi_pool(feat, [proposals],\n",
    "                                                           output_size=self.pool_size,\n",
    "                                                           spatial_scale=possible_scales[0])\n",
    "        proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n",
    "        box_fc_6 = torch.nn.functional.relu(self.fc6(proposal_roi_pool_feats))\n",
    "        box_fc_7 = torch.nn.functional.relu(self.fc7(box_fc_6))\n",
    "        cls_scores = self.cls_layer(box_fc_7)\n",
    "        box_transform_pred = self.bbox_reg_layer(box_fc_7)\n",
    "        # cls_scores -> (proposals, num_classes)\n",
    "        # box_transform_pred -> (proposals, num_classes * 4)\n",
    "        ##############################################\n",
    "        \n",
    "        num_boxes, num_classes = cls_scores.shape\n",
    "        # This should match the total elements in box_transform_pred\n",
    "        box_transform_pred = box_transform_pred.reshape(num_boxes, num_classes, 4)\n",
    "        frcnn_output = {}\n",
    "        if self.training and target is not None:\n",
    "            classification_loss = torch.nn.functional.cross_entropy(cls_scores, labels)\n",
    "            \n",
    "            # Compute localization loss only for non-background labelled proposals\n",
    "            fg_proposals_idxs = torch.where(labels > 0)[0]\n",
    "            # Get class labels for these positive proposals\n",
    "            fg_cls_labels = labels[fg_proposals_idxs]\n",
    "            \n",
    "            localization_loss = torch.nn.functional.smooth_l1_loss(\n",
    "                box_transform_pred[fg_proposals_idxs, fg_cls_labels],\n",
    "                regression_targets[fg_proposals_idxs],\n",
    "                beta=1/9,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "            localization_loss = localization_loss / labels.numel()\n",
    "            frcnn_output['frcnn_classification_loss'] = classification_loss\n",
    "            frcnn_output['frcnn_localization_loss'] = localization_loss\n",
    "        \n",
    "        if self.training:\n",
    "            return frcnn_output\n",
    "        else:\n",
    "            device = cls_scores.device\n",
    "            # Apply transformation predictions to proposals\n",
    "            pred_boxes = apply_regression_pred_to_anchors_or_proposals(box_transform_pred, proposals)\n",
    "            pred_scores = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
    "            \n",
    "            # Clamp box to image boundary\n",
    "            pred_boxes = clamp_boxes_to_image_boundary(pred_boxes, image_shape)\n",
    "            \n",
    "            # create labels for each prediction\n",
    "            pred_labels = torch.arange(num_classes, device=device)\n",
    "            pred_labels = pred_labels.view(1, -1).expand_as(pred_scores)\n",
    "            \n",
    "            # remove predictions with the background label\n",
    "            pred_boxes = pred_boxes[:, 1:]\n",
    "            pred_scores = pred_scores[:, 1:]\n",
    "            pred_labels = pred_labels[:, 1:]\n",
    "            \n",
    "            # pred_boxes -> (number_proposals, num_classes-1, 4)\n",
    "            # pred_scores -> (number_proposals, num_classes-1)\n",
    "            # pred_labels -> (number_proposals, num_classes-1)\n",
    "            \n",
    "            # batch everything, by making every class prediction be a separate instance\n",
    "            pred_boxes = pred_boxes.reshape(-1, 4)\n",
    "            pred_scores = pred_scores.reshape(-1)\n",
    "            pred_labels = pred_labels.reshape(-1)\n",
    "            \n",
    "            pred_boxes, pred_labels, pred_scores = self.filter_predictions(pred_boxes, pred_labels, pred_scores)\n",
    "            frcnn_output['boxes'] = pred_boxes\n",
    "            frcnn_output['scores'] = pred_scores\n",
    "            frcnn_output['labels'] = pred_labels\n",
    "            return frcnn_output\n",
    "    \n",
    "    def filter_predictions(self, pred_boxes, pred_labels, pred_scores):\n",
    "        r\"\"\"\n",
    "        Method to filter predictions by applying the following in order:\n",
    "        1. Filter low scoring boxes\n",
    "        2. Remove small size boxesâˆ‚\n",
    "        3. NMS for each class separately\n",
    "        4. Keep only topK detections\n",
    "        :param pred_boxes:\n",
    "        :param pred_labels:\n",
    "        :param pred_scores:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # remove low scoring boxes\n",
    "        keep = torch.where(pred_scores > self.low_score_threshold)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        # Remove small boxes\n",
    "        min_size = 16\n",
    "        ws, hs = pred_boxes[:, 2] - pred_boxes[:, 0], pred_boxes[:, 3] - pred_boxes[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        # Class wise nms\n",
    "        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n",
    "        for class_id in torch.unique(pred_labels):\n",
    "            curr_indices = torch.where(pred_labels == class_id)[0]\n",
    "            curr_keep_indices = torch.ops.torchvision.nms(pred_boxes[curr_indices],\n",
    "                                                          pred_scores[curr_indices],\n",
    "                                                          self.nms_threshold)\n",
    "            keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        post_nms_keep_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n",
    "        keep = post_nms_keep_indices[:self.topK_detections]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        return pred_boxes, pred_labels, pred_scores\n",
    "\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, model_config, num_classes):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.model_config = model_config\n",
    "        vgg16 = torchvision.models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        self.backbone = vgg16.features[:-1]\n",
    "        self.rpn = RegionProposalNetwork(model_config['backbone_out_channels'],\n",
    "                                         scales=model_config['scales'],\n",
    "                                         aspect_ratios=model_config['aspect_ratios'],\n",
    "                                         model_config=model_config)\n",
    "        self.roi_head = ROIHead(model_config, num_classes, in_channels=model_config['backbone_out_channels'])\n",
    "        for layer in self.backbone[:10]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.image_mean = [0.485, 0.456, 0.406]\n",
    "        self.image_std = [0.229, 0.224, 0.225]\n",
    "        self.min_size = model_config['min_im_size']\n",
    "        self.max_size = model_config['max_im_size']\n",
    "    \n",
    "    def normalize_resize_image_and_boxes(self, image, bboxes):\n",
    "        dtype, device = image.dtype, image.device\n",
    "        \n",
    "        # Normalize\n",
    "        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device)\n",
    "        std = torch.as_tensor(self.image_std, dtype=dtype, device=device)\n",
    "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "        #############\n",
    "        \n",
    "        # Resize to 1000x600 such that lowest size dimension is scaled upto 600\n",
    "        # but larger dimension is not more than 1000\n",
    "        # So compute scale factor for both and scale is minimum of these two\n",
    "        h, w = image.shape[-2:]\n",
    "        im_shape = torch.tensor(image.shape[-2:])\n",
    "        min_size = torch.min(im_shape).to(dtype=torch.float32)\n",
    "        max_size = torch.max(im_shape).to(dtype=torch.float32)\n",
    "        scale = torch.min(float(self.min_size) / min_size, float(self.max_size) / max_size)\n",
    "        scale_factor = scale.item()\n",
    "        \n",
    "        # Resize image based on scale computed\n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image,\n",
    "            size=None,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=\"bilinear\",\n",
    "            recompute_scale_factor=True,\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        if bboxes is not None:\n",
    "            # Resize boxes by\n",
    "            ratios = [\n",
    "                torch.tensor(s, dtype=torch.float32, device=bboxes.device)\n",
    "                / torch.tensor(s_orig, dtype=torch.float32, device=bboxes.device)\n",
    "                for s, s_orig in zip(image.shape[-2:], (h, w))\n",
    "            ]\n",
    "            ratio_height, ratio_width = ratios\n",
    "            xmin, ymin, xmax, ymax = bboxes.unbind(2)\n",
    "            xmin = xmin * ratio_width\n",
    "            xmax = xmax * ratio_width\n",
    "            ymin = ymin * ratio_height\n",
    "            ymax = ymax * ratio_height\n",
    "            bboxes = torch.stack((xmin, ymin, xmax, ymax), dim=2)\n",
    "        return image, bboxes\n",
    "    \n",
    "    def forward(self, image, target=None):\n",
    "        old_shape = image.shape[-2:]\n",
    "        if self.training:\n",
    "            # Normalize and resize boxes\n",
    "            image, bboxes = self.normalize_resize_image_and_boxes(image, target['bboxes'])\n",
    "            target['bboxes'] = bboxes\n",
    "        else:\n",
    "            image, _ = self.normalize_resize_image_and_boxes(image, None)\n",
    "        \n",
    "        # Call backbone\n",
    "        feat = self.backbone(image)\n",
    "        \n",
    "        # Call RPN and get proposals\n",
    "        rpn_output = self.rpn(image, feat, target)\n",
    "        proposals = rpn_output['proposals']\n",
    "        \n",
    "        # Call ROI head and convert proposals to boxes\n",
    "        frcnn_output = self.roi_head(feat, proposals, image.shape[-2:], target)\n",
    "        if not self.training:\n",
    "            # Transform boxes to original image dimensions called only during inference\n",
    "            frcnn_output['boxes'] = transform_boxes_to_original_size(frcnn_output['boxes'],\n",
    "                                                                     image.shape[-2:],\n",
    "                                                                     old_shape)\n",
    "        return rpn_output, frcnn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T20:22:53.819136Z",
     "iopub.status.busy": "2024-11-29T20:22:53.818864Z",
     "iopub.status.idle": "2024-11-29T20:22:53.831670Z",
     "shell.execute_reply": "2024-11-29T20:22:53.830691Z",
     "shell.execute_reply.started": "2024-11-29T20:22:53.819111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(args):    \n",
    "    dataset_config = config['dataset_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "    \n",
    "    seed = train_config['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    voc = VOCDataset('train',\n",
    "                     im_dir=dataset_config['im_train_path'],\n",
    "                     ann_dir=dataset_config['ann_train_path'],\n",
    "                     max_images=config[\"dataset_params\"][\"max_training_samples\"])\n",
    "    train_dataset = DataLoader(voc,\n",
    "                               batch_size=1,\n",
    "                               shuffle=True,\n",
    "                               num_workers=0)\n",
    "    \n",
    "    faster_rcnn_model = FasterRCNN(model_config,\n",
    "                                   num_classes=dataset_config['num_classes'])\n",
    "    faster_rcnn_model.train()\n",
    "    faster_rcnn_model.to(device)\n",
    "\n",
    "    if not os.path.exists(train_config['task_name']):\n",
    "        os.mkdir(train_config['task_name'])\n",
    "    optimizer = torch.optim.SGD(lr=train_config['lr'],\n",
    "                                params=filter(lambda p: p.requires_grad,\n",
    "                                              faster_rcnn_model.parameters()),\n",
    "                                weight_decay=5E-4,\n",
    "                                momentum=0.9)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=train_config['lr_steps'], gamma=0.1)\n",
    "    \n",
    "    acc_steps = train_config['acc_steps']\n",
    "    num_epochs = train_config['num_epochs']\n",
    "    step_count = 1\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        rpn_classification_losses = []\n",
    "        rpn_localization_losses = []\n",
    "        frcnn_classification_losses = []\n",
    "        frcnn_localization_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for im, target, fname in tqdm(train_dataset):\n",
    "            im = im.float().to(device)\n",
    "            target['bboxes'] = target['bboxes'].float().to(device)\n",
    "            target['labels'] = target['labels'].long().to(device)\n",
    "            rpn_output, frcnn_output = faster_rcnn_model(im, target)\n",
    "            \n",
    "            rpn_loss = rpn_output['rpn_classification_loss'] + rpn_output['rpn_localization_loss']\n",
    "            frcnn_loss = frcnn_output['frcnn_classification_loss'] + frcnn_output['frcnn_localization_loss']\n",
    "            loss = rpn_loss + frcnn_loss\n",
    "            \n",
    "            rpn_classification_losses.append(rpn_output['rpn_classification_loss'].item())\n",
    "            rpn_localization_losses.append(rpn_output['rpn_localization_loss'].item())\n",
    "            frcnn_classification_losses.append(frcnn_output['frcnn_classification_loss'].item())\n",
    "            frcnn_localization_losses.append(frcnn_output['frcnn_localization_loss'].item())\n",
    "            loss = loss / acc_steps\n",
    "            loss.backward()\n",
    "            if step_count % acc_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            step_count += 1\n",
    "        print('Finished epoch {}'.format(i))\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.save(faster_rcnn_model.state_dict(), os.path.join(train_config['task_name'],\n",
    "                                                                train_config['ckpt_name']))\n",
    "        loss_output = ''\n",
    "        loss_output += 'RPN Classification Loss : {:.4f}'.format(np.mean(rpn_classification_losses))\n",
    "        loss_output += ' | RPN Localization Loss : {:.4f}'.format(np.mean(rpn_localization_losses))\n",
    "        loss_output += ' | FRCNN Classification Loss : {:.4f}'.format(np.mean(frcnn_classification_losses))\n",
    "        loss_output += ' | FRCNN Localization Loss : {:.4f}'.format(np.mean(frcnn_localization_losses))\n",
    "        print(loss_output)\n",
    "        scheduler.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print('Training completed in {:.2f} seconds.'.format(elapsed_time))\n",
    "    return faster_rcnn_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "def get_iou_inf(det, gt):\n",
    "    det_x1, det_y1, det_x2, det_y2 = det\n",
    "    gt_x1, gt_y1, gt_x2, gt_y2 = gt\n",
    "    \n",
    "    x_left = max(det_x1, gt_x1)\n",
    "    y_top = max(det_y1, gt_y1)\n",
    "    x_right = min(det_x2, gt_x2)\n",
    "    y_bottom = min(det_y2, gt_y2)\n",
    "    \n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    area_intersection = (x_right - x_left) * (y_bottom - y_top)\n",
    "    det_area = (det_x2 - det_x1) * (det_y2 - det_y1)\n",
    "    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)\n",
    "    area_union = float(det_area + gt_area - area_intersection + 1E-6)\n",
    "    iou = area_intersection / area_union\n",
    "    return iou\n",
    "\n",
    "\n",
    "def compute_map(det_boxes, gt_boxes, iou_threshold=0.5, method='area'):    \n",
    "    gt_labels = {cls_key for im_gt in gt_boxes for cls_key in im_gt.keys()}\n",
    "    gt_labels = sorted(gt_labels)\n",
    "    all_aps = {}\n",
    "    # average precisions for ALL classes\n",
    "    aps = []\n",
    "    for idx, label in enumerate(gt_labels):\n",
    "        # Get detection predictions of this class\n",
    "        cls_dets = [\n",
    "            [im_idx, im_dets_label] for im_idx, im_dets in enumerate(det_boxes)\n",
    "            if label in im_dets for im_dets_label in im_dets[label]\n",
    "        ]\n",
    "        \n",
    "        # Sort them by confidence score\n",
    "        cls_dets = sorted(cls_dets, key=lambda k: -k[1][-1])\n",
    "        \n",
    "        # For tracking which gt boxes of this class have already been matched\n",
    "        gt_matched = [[False for _ in im_gts[label]] for im_gts in gt_boxes]\n",
    "        # Number of gt boxes for this class for recall calculation\n",
    "        num_gts = sum([len(im_gts[label]) for im_gts in gt_boxes])\n",
    "        tp = [0] * len(cls_dets)\n",
    "        fp = [0] * len(cls_dets)\n",
    "        \n",
    "        # For each prediction\n",
    "        for det_idx, (im_idx, det_pred) in enumerate(cls_dets):\n",
    "            # Get gt boxes for this image and this label\n",
    "            im_gts = gt_boxes[im_idx][label]\n",
    "            max_iou_found = -1\n",
    "            max_iou_gt_idx = -1\n",
    "            \n",
    "            # Get best matching gt box\n",
    "            for gt_box_idx, gt_box in enumerate(im_gts):\n",
    "                gt_box_iou = get_iou_inf(det_pred[:-1], gt_box)\n",
    "                if gt_box_iou > max_iou_found:\n",
    "                    max_iou_found = gt_box_iou\n",
    "                    max_iou_gt_idx = gt_box_idx\n",
    "            # TP only if iou >= threshold and this gt has not yet been matched\n",
    "            if max_iou_found < iou_threshold or gt_matched[im_idx][max_iou_gt_idx]:\n",
    "                fp[det_idx] = 1\n",
    "            else:\n",
    "                tp[det_idx] = 1\n",
    "                # If tp then we set this gt box as matched\n",
    "                gt_matched[im_idx][max_iou_gt_idx] = True\n",
    "        # Cumulative tp and fp\n",
    "        tp = np.cumsum(tp)\n",
    "        fp = np.cumsum(fp)\n",
    "        \n",
    "        eps = np.finfo(np.float32).eps\n",
    "        recalls = tp / np.maximum(num_gts, eps)\n",
    "        precisions = tp / np.maximum((tp + fp), eps)\n",
    "\n",
    "        if method == 'area':\n",
    "            recalls = np.concatenate(([0.0], recalls, [1.0]))\n",
    "            precisions = np.concatenate(([0.0], precisions, [0.0]))\n",
    "            \n",
    "            # Replace precision values with recall r with maximum precision value\n",
    "            # of any recall value >= r\n",
    "            # This computes the precision envelope\n",
    "            for i in range(precisions.size - 1, 0, -1):\n",
    "                precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])\n",
    "            # For computing area, get points where recall changes value\n",
    "            i = np.where(recalls[1:] != recalls[:-1])[0]\n",
    "            # Add the rectangular areas to get ap\n",
    "            ap = np.sum((recalls[i + 1] - recalls[i]) * precisions[i + 1])\n",
    "        elif method == 'interp':\n",
    "            ap = 0.0\n",
    "            for interp_pt in np.arange(0, 1 + 1E-3, 0.1):\n",
    "                # Get precision values for recall values >= interp_pt\n",
    "                prec_interp_pt = precisions[recalls >= interp_pt]\n",
    "                \n",
    "                # Get max of those precision values\n",
    "                prec_interp_pt = prec_interp_pt.max() if prec_interp_pt.size > 0.0 else 0.0\n",
    "                ap += prec_interp_pt\n",
    "            ap = ap / 11.0\n",
    "        else:\n",
    "            raise ValueError('Method can only be area or interp')\n",
    "        if num_gts > 0:\n",
    "            aps.append(ap)\n",
    "            all_aps[label] = ap\n",
    "        else:\n",
    "            all_aps[label] = np.nan\n",
    "    # compute mAP at provided iou threshold\n",
    "    mean_ap = sum(aps) / len(aps)\n",
    "    return mean_ap, all_aps\n",
    "\n",
    "\n",
    "def load_model_and_dataset(args):    \n",
    "    dataset_config = config['dataset_params']\n",
    "    model_config = config['model_params']\n",
    "    train_config = config['train_params']\n",
    "    \n",
    "    seed = train_config['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    voc = VOCDataset('test', im_dir=dataset_config['im_test_path'], ann_dir=dataset_config['ann_test_path'], max_images=config[\"dataset_params\"][\"max_testing_samples\"])\n",
    "    test_dataset = DataLoader(voc, batch_size=1, shuffle=False)\n",
    "\n",
    "    if 'model' in args and args['model'] is not None:\n",
    "        print(\"Using the trained model passed in infer_args.\")\n",
    "        faster_rcnn_model = args['model']\n",
    "        faster_rcnn_model.eval()\n",
    "    else: \n",
    "        faster_rcnn_model = FasterRCNN(model_config, num_classes=dataset_config['num_classes'])\n",
    "        faster_rcnn_model.eval()\n",
    "        faster_rcnn_model.to(device)\n",
    "        faster_rcnn_model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n",
    "                                                                train_config['ckpt_name']),\n",
    "                                                    map_location=device))\n",
    "    return faster_rcnn_model, voc, test_dataset\n",
    "\n",
    "\n",
    "def infer(args):\n",
    "    dataset_config = config['dataset_params']\n",
    "    \n",
    "    if not os.path.exists('samples'):\n",
    "        os.mkdir('samples')\n",
    "\n",
    "    # Use the provided model if exists, otherwise load it\n",
    "    if hasattr(args, 'model'):\n",
    "        faster_rcnn_model = args.model\n",
    "        voc = VOCDataset('test', im_dir=dataset_config['im_test_path'], ann_dir=dataset_config['ann_test_path'], max_images=config[\"dataset_params\"][\"max_testing_samples\"])\n",
    "        test_dataset = DataLoader(voc, batch_size=1, shuffle=False)\n",
    "    else:\n",
    "        faster_rcnn_model, voc, test_dataset = load_model_and_dataset(args)\n",
    "    \n",
    "    # Hard coding the low score threshold for inference on images for now\n",
    "    # Should come from config\n",
    "    faster_rcnn_model.roi_head.low_score_threshold = 0.7\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for sample_count in tqdm(range(10)):\n",
    "        random_idx = random.randint(0, len(voc))\n",
    "        im, target, fname = voc[random_idx]\n",
    "        im = im.unsqueeze(0).float().to(device)\n",
    "\n",
    "        gt_im = cv2.imread(fname)\n",
    "        gt_im_copy = gt_im.copy()\n",
    "        \n",
    "        # Saving images with ground truth boxes\n",
    "        for idx, box in enumerate(target['bboxes']):\n",
    "            \n",
    "            cv2.rectangle(gt_im, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])\n",
    "            cv2.rectangle(gt_im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])\n",
    "            text = voc.idx2label[target['labels'][idx].detach().cpu().item()]\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n",
    "            text_w, text_h = text_size\n",
    "            cv2.rectangle(gt_im_copy , (x1, y1), (x1 + 10+text_w, y1 + 10+text_h), [255, 255, 255], -1)\n",
    "            cv2.putText(gt_im, text=voc.idx2label[target['labels'][idx].detach().cpu().item()],\n",
    "                        org=(x1+5, y1+15),\n",
    "                        thickness=1,\n",
    "                        fontScale=1,\n",
    "                        color=[0, 0, 0],\n",
    "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
    "            cv2.putText(gt_im_copy, text=text,\n",
    "                        org=(x1 + 5, y1 + 15),\n",
    "                        thickness=1,\n",
    "                        fontScale=1,\n",
    "                        color=[0, 0, 0],\n",
    "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
    "        cv2.addWeighted(gt_im_copy, 0.7, gt_im, 0.3, 0, gt_im)\n",
    "        cv2.imwrite('samples/output_frcnn_gt_{}.png'.format(sample_count), gt_im)\n",
    "        \n",
    "        # Getting predictions from trained model\n",
    "        rpn_output, frcnn_output = faster_rcnn_model(im, None)\n",
    "        boxes = frcnn_output['boxes']\n",
    "        labels = frcnn_output['labels']\n",
    "        scores = frcnn_output['scores']\n",
    "        im = cv2.imread(fname)\n",
    "        im_copy = im.copy()\n",
    "        \n",
    "        # Saving images with predicted boxes\n",
    "        for idx, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            cv2.rectangle(im, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])\n",
    "            cv2.rectangle(im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])\n",
    "            text = '{} : {:.2f}'.format(voc.idx2label[labels[idx].detach().cpu().item()],\n",
    "                                        scores[idx].detach().cpu().item())\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n",
    "            text_w, text_h = text_size\n",
    "            cv2.rectangle(im_copy , (x1, y1), (x1 + 10+text_w, y1 + 10+text_h), [255, 255, 255], -1)\n",
    "            cv2.putText(im, text=text,\n",
    "                        org=(x1+5, y1+15),\n",
    "                        thickness=1,\n",
    "                        fontScale=1,\n",
    "                        color=[0, 0, 0],\n",
    "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
    "            cv2.putText(im_copy, text=text,\n",
    "                        org=(x1 + 5, y1 + 15),\n",
    "                        thickness=1,\n",
    "                        fontScale=1,\n",
    "                        color=[0, 0, 0],\n",
    "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
    "        cv2.addWeighted(im_copy, 0.7, im, 0.3, 0, im)\n",
    "        cv2.imwrite('samples/output_frcnn_{}.jpg'.format(sample_count), im)\n",
    "\n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print('Inference completed in {:.2f} seconds.'.format(elapsed_time))\n",
    "\n",
    "\n",
    "def evaluate_map(args):\n",
    "    faster_rcnn_model, voc, test_dataset = load_model_and_dataset(args)\n",
    "    gts = []\n",
    "    preds = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for im, target, fname in tqdm(test_dataset):\n",
    "            im_name = fname\n",
    "            im = im.float().to(device)\n",
    "            target_boxes = target['bboxes'].float().to(device)[0]\n",
    "            target_labels = target['labels'].long().to(device)[0]\n",
    "            rpn_output, frcnn_output = faster_rcnn_model(im, None)\n",
    "\n",
    "            boxes = frcnn_output['boxes']\n",
    "            labels = frcnn_output['labels']\n",
    "            scores = frcnn_output['scores']\n",
    "            \n",
    "            pred_boxes = {}\n",
    "            gt_boxes = {}\n",
    "            for label_name in voc.label2idx:\n",
    "                pred_boxes[label_name] = []\n",
    "                gt_boxes[label_name] = []\n",
    "            \n",
    "            for idx, box in enumerate(boxes):\n",
    "                x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
    "                label = labels[idx].detach().cpu().item()\n",
    "                score = scores[idx].detach().cpu().item()\n",
    "                label_name = voc.idx2label[label]\n",
    "                pred_boxes[label_name].append([x1, y1, x2, y2, score])\n",
    "            for idx, box in enumerate(target_boxes):\n",
    "                x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
    "                label = target_labels[idx].detach().cpu().item()\n",
    "                label_name = voc.idx2label[label]\n",
    "                gt_boxes[label_name].append([x1, y1, x2, y2])\n",
    "            \n",
    "            gts.append(gt_boxes)\n",
    "            preds.append(pred_boxes)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print('Inference completed in {:.2f} seconds.'.format(elapsed_time))\n",
    "   \n",
    "    mean_ap, all_aps = compute_map(preds, gts, method='interp')\n",
    "    print('Class Wise Average Precisions')\n",
    "    for idx in range(len(voc.idx2label)):\n",
    "        print('AP for class {} = {:.4f}'.format(voc.idx2label[idx], all_aps[voc.idx2label[idx]]))\n",
    "    print('Mean Average Precision : {:.4f}'.format(mean_ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T20:22:54.057708Z",
     "iopub.status.busy": "2024-11-29T20:22:54.057474Z",
     "iopub.status.idle": "2024-11-29T20:23:54.802358Z",
     "shell.execute_reply": "2024-11-29T20:23:54.801412Z",
     "shell.execute_reply.started": "2024-11-29T20:22:54.057686Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'background', 1: 'aeroplane', 2: 'bicycle', 3: 'bird', 4: 'boat', 5: 'bottle', 6: 'bus', 7: 'car', 8: 'cat', 9: 'chair', 10: 'cow', 11: 'diningtable', 12: 'dog', 13: 'horse', 14: 'motorbike', 15: 'person', 16: 'pottedplant', 17: 'sheep', 18: 'sofa', 19: 'train', 20: 'tvmonitor'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5011/5011 [00:00<00:00, 27362.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 5011 images found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:35<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0\n",
      "RPN Classification Loss : 0.3632 | RPN Localization Loss : 0.1360 | FRCNN Classification Loss : 0.7048 | FRCNN Localization Loss : 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:38<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1\n",
      "RPN Classification Loss : 0.2837 | RPN Localization Loss : 0.1245 | FRCNN Classification Loss : 0.6571 | FRCNN Localization Loss : 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:28<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 2\n",
      "RPN Classification Loss : 0.2642 | RPN Localization Loss : 0.1219 | FRCNN Classification Loss : 0.6155 | FRCNN Localization Loss : 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:26<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 3\n",
      "RPN Classification Loss : 0.2584 | RPN Localization Loss : 0.1198 | FRCNN Classification Loss : 0.6034 | FRCNN Localization Loss : 0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:26<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 4\n",
      "RPN Classification Loss : 0.2522 | RPN Localization Loss : 0.1191 | FRCNN Classification Loss : 0.5886 | FRCNN Localization Loss : 0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:21<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 5\n",
      "RPN Classification Loss : 0.2515 | RPN Localization Loss : 0.1188 | FRCNN Classification Loss : 0.5831 | FRCNN Localization Loss : 0.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:24<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 6\n",
      "RPN Classification Loss : 0.2481 | RPN Localization Loss : 0.1172 | FRCNN Classification Loss : 0.5721 | FRCNN Localization Loss : 0.0647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:23<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 7\n",
      "RPN Classification Loss : 0.2451 | RPN Localization Loss : 0.1165 | FRCNN Classification Loss : 0.5698 | FRCNN Localization Loss : 0.0628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:15<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 8\n",
      "RPN Classification Loss : 0.2450 | RPN Localization Loss : 0.1165 | FRCNN Classification Loss : 0.5702 | FRCNN Localization Loss : 0.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:11<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 9\n",
      "RPN Classification Loss : 0.2466 | RPN Localization Loss : 0.1157 | FRCNN Classification Loss : 0.5603 | FRCNN Localization Loss : 0.0609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:17:22<00:00,  3.09s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 10\n",
      "RPN Classification Loss : 0.2408 | RPN Localization Loss : 0.1157 | FRCNN Classification Loss : 0.5518 | FRCNN Localization Loss : 0.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:16:51<00:00,  3.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 11\n",
      "RPN Classification Loss : 0.2398 | RPN Localization Loss : 0.1162 | FRCNN Classification Loss : 0.5504 | FRCNN Localization Loss : 0.0595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:16:37<00:00,  3.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 12\n",
      "RPN Classification Loss : 0.1866 | RPN Localization Loss : 0.0994 | FRCNN Classification Loss : 0.4859 | FRCNN Localization Loss : 0.0564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:16:39<00:00,  3.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 13\n",
      "RPN Classification Loss : 0.1712 | RPN Localization Loss : 0.0965 | FRCNN Classification Loss : 0.4680 | FRCNN Localization Loss : 0.0552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:16:40<00:00,  3.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 14\n",
      "RPN Classification Loss : 0.1643 | RPN Localization Loss : 0.0954 | FRCNN Classification Loss : 0.4565 | FRCNN Localization Loss : 0.0543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:16:42<00:00,  3.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 15\n",
      "RPN Classification Loss : 0.1585 | RPN Localization Loss : 0.0946 | FRCNN Classification Loss : 0.4506 | FRCNN Localization Loss : 0.0539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:16:42<00:00,  3.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 16\n",
      "RPN Classification Loss : 0.1479 | RPN Localization Loss : 0.0912 | FRCNN Classification Loss : 0.4355 | FRCNN Localization Loss : 0.0530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:16:39<00:00,  3.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 17\n",
      "RPN Classification Loss : 0.1457 | RPN Localization Loss : 0.0909 | FRCNN Classification Loss : 0.4347 | FRCNN Localization Loss : 0.0529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:16:43<00:00,  3.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 18\n",
      "RPN Classification Loss : 0.1448 | RPN Localization Loss : 0.0907 | FRCNN Classification Loss : 0.4338 | FRCNN Localization Loss : 0.0527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [4:16:39<00:00,  3.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 19\n",
      "RPN Classification Loss : 0.1441 | RPN Localization Loss : 0.0905 | FRCNN Classification Loss : 0.4328 | FRCNN Localization Loss : 0.0528\n",
      "Training completed in 308521.11 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_args = {\n",
    "    'config_path': 'config/voc.yaml'\n",
    "}\n",
    "\n",
    "model = train(train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T20:24:58.116101Z",
     "iopub.status.busy": "2024-11-29T20:24:58.115177Z",
     "iopub.status.idle": "2024-11-29T20:25:01.012922Z",
     "shell.execute_reply": "2024-11-29T20:25:01.011789Z",
     "shell.execute_reply.started": "2024-11-29T20:24:58.116055Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'background', 1: 'aeroplane', 2: 'bicycle', 3: 'bird', 4: 'boat', 5: 'bottle', 6: 'bus', 7: 'car', 8: 'cat', 9: 'chair', 10: 'cow', 11: 'diningtable', 12: 'dog', 13: 'horse', 14: 'motorbike', 15: 'person', 16: 'pottedplant', 17: 'sheep', 18: 'sofa', 19: 'train', 20: 'tvmonitor'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<00:00, 42989.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 4952 images found\n",
      "Using the trained model passed in infer_args.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [1:10:19<00:00, 16.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed in 4219.52 seconds.\n",
      "Class Wise Average Precisions\n",
      "AP for class background = nan\n",
      "AP for class aeroplane = 0.3055\n",
      "AP for class bicycle = 0.2160\n",
      "AP for class bird = 0.0914\n",
      "AP for class boat = 0.2441\n",
      "AP for class bottle = 0.0000\n",
      "AP for class bus = 0.3336\n",
      "AP for class car = 0.5046\n",
      "AP for class cat = 0.3362\n",
      "AP for class chair = 0.2326\n",
      "AP for class cow = 0.2906\n",
      "AP for class diningtable = 0.0000\n",
      "AP for class dog = 0.2867\n",
      "AP for class horse = 0.4935\n",
      "AP for class motorbike = 0.3328\n",
      "AP for class person = 0.6095\n",
      "AP for class pottedplant = 0.0153\n",
      "AP for class sheep = 0.0356\n",
      "AP for class sofa = 0.0596\n",
      "AP for class train = 0.3976\n",
      "AP for class tvmonitor = 0.2862\n",
      "Mean Average Precision : 0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "infer_args = {\n",
    "    'config_path': 'config/voc.yaml',\n",
    "    'evaluate': True,\n",
    "    'infer_samples': False,\n",
    "    'model': model\n",
    "}\n",
    "\n",
    "# Make inference\n",
    "evaluate_map(infer_args)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 18276,
     "sourceId": 23902,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "qcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
